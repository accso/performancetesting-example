# Konfiguration, um die angegebene Beispieldatei vollständig an Elasticsearch zu übergeben
# Voraussetzung:
# - Elasticsearch-Host ist unter "elasticsearch:9200" erreichbar
# Die Eingabedaten müssen als JSON-Zeilen vorliegen. Jede Zeile ist ein "Event".
# Sämtliche Felder einer Zeile gehen ungefiltert an Elasticsearch.
# Indexierungsverfolgung per Elasticsearch-Count-API:
# - mit curl: curl -XGET 'http://elasticsearch:9200/logstash-exampledata/_count'
# - mit Kibana-Sense: GET /logstash-exampledata/_count
# Zur Wiederholung der Indexierung:
# - logstash-Prozess abbrechen
# - sincedb-Datei löschen
# - Elasticsearch-Index löschen
# -- mit curl: curl -XDELETE 'http://elasticsearch:9200/logstash-exampledata'
# -- mit Kibana-Sense: DELETE /logstash-exampledata
input {
	# Plugin-Ergebnisse: Eingabezeile in message, Verarbeitungszeitpunkt in @timestamp, Dateipfad in path, Rechnername in host
	file {
		path => "/logstash/logs/*.log"
		sincedb_path => "/logstash/sincedb_applicationdata"
		# immer die gesamte Datei verarbeiten, anstatt nach dem Start auf neue Einträge zu warten
		start_position => "beginning"
		# Sekundenangabe (Standard nur 24 Stunden), um zu Testzwecken auch ältere Eingabedateien (bis 365 Tage) zu erfassen 
		ignore_older => 31536000
	}
}
filter {
	# erzeugt die in message enthaltenen JSON-Felder oder ein Fehler-Tag  
	json {
		source => "message"
	}
	# entfernt Events mit Parse-Fehlern
	if "_jsonparsefailure" in [tags] {
		drop {}
	}
}
output {
# zu Testzwecken JSON-artige Ausgabe
#	stdout { codec => rubydebug }
  
	# Indexierung mit dem Standard-Template "elasticsearch-template.json" und dem Standard-Typ "logs" 
	elasticsearch {	
		hosts => ["elasticsearch"]
	}
}